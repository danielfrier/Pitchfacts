{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import codecs\n",
    "import nltk\n",
    "import multiprocessing\n",
    "\n",
    "from nltk.corpus import *\n",
    "from collections import *\n",
    "from gensim import *\n",
    "from gensim.models import *\n",
    "from sklearn import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from nltk.stem import *\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    stopwords = [word for line in f for word in line.split()]\n",
    "df = pd.read_csv('data/p4kreviews.csv', skipinitialspace=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    temp = text\n",
    "    temp=re.sub('\\S+[\\'\\’\\‘]\\S+','',(temp))\n",
    "    temp=re.sub('\\w*\\d\\S*','',(temp))\n",
    "    temp=re.sub('(?<!^|$)(?<!([(\\.)(\\!)(\\?)(\\“)]\\s))([A-Z]\\S+)','',(temp))\n",
    "    temp=re.sub('[^A-z\\s\\-\\–\\&]',' ',(temp))\n",
    "    temp=re.sub('\\su\\ss\\s',' U.S. ',(temp))\n",
    "    temp=re.sub('\\[#.+]\\|+','',(temp))\n",
    "    temp = temp.lower()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [str(re.sub('—',' ',str(df.loc[i].review))).decode('unicode_escape').encode('ascii','ignore') for i in list(range(len(df.loc[:])))]\n",
    "total_text = ' '.join([reviews[i] for i in list(range(len(reviews)))])\n",
    "cleaned_orig_reviews = [clean_text(text) for text in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(m):\n",
    "    return '' if m.group() in stopwords else m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [ re.sub(r'\\w+(\\-|\\—|\\.|\\&|\\’)?(\\w+)?', remove_stop, review) for review in reviews]\n",
    "reviews = [ re.sub(r'\\-+\\s', ' ', review) for review in reviews]\n",
    "total_text = ' '.join([reviews[i] for i in list(range(len(reviews)))])\n",
    "total_words = len(total_text)\n",
    "vocab_counter = Counter(total_text.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences = [re.split('[\\.\\!\\?]\\s',re.sub('[^A-z\\.\\!\\?\\-\\&]+',' ',review.lower())) for review in reviews]\n",
    " tempences = []\n",
    " for i in list(range(len(sentences))):\n",
    "     tempences.append(list(filter(None, [o.split() for o in sentences[i]])))\n",
    " sentences = tempences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rawdata = {'doc_id':[],'review':[]}\n",
    "for i in list(range(len(reviews))):\n",
    "    tempreview = str(reviews[i])\n",
    "    tempreview = clean_text(tempreview)\n",
    "    tempreview = str(' '.join([lemmatizer.lemmatize(tag[0], get_wordnet_pos(tag[1])) for tag in pos_tag(tempreview.split())]))\n",
    "    rawdata['doc_id'].append(i)\n",
    "    rawdata['review'].append(tempreview)\n",
    "rawdata['review'] = [ re.sub(r'\\w+(\\-|\\—|\\.|\\&|\\’)?(\\w+)?', remove_stop, review) for review in rawdata['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df2 = pd.DataFrame(rawdata,columns=['doc_id','review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df2.to_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metadata = df.drop('review',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metadata['doc_id'] = rawdata['doc_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metadata.to_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documents = rawdata['review']\n",
    "texts = [[word for word in document.lower().split() if word not in stopwords]\n",
    "          for document in documents]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "         frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "    for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('reviewsDict.dict')\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('reviewsDict.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documents = cleaned_orig_reviews\n",
    "texts = [[word for word in document.lower().split() if word not in stopwords]\n",
    "          for document in documents]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "         frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "    for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('models/orig_text.dict')\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('models/orig_text.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwords= set(stopwords.words('english'))\n",
    "stopwords = [i.encode('ascii','ignore') for i in list(stopwords)]\n",
    "stopwords = set(stopwords)\n",
    "stopwords |= set(['music','album','albums','band','bands','artist','artists','song','songs',\n",
    "                  'track','tracks','time','length','\\\\x','year','years','would','something',\n",
    "                  'time','often','yet','another','title','seems','words','well','really','right',\n",
    "                  'thing','enough','got','know','say','released','people',\n",
    "                  'best','new','reissue','since','less','go','label'])\n",
    "counter = Counter(total_text.lower().split(' '))\n",
    "ignore = list(stopwords)\n",
    "for word in list(counter):\n",
    "    if word in ignore:\n",
    "        del counter[word]\n",
    "stopwords |= set([counter.most_common(80)[i][0] for i in list(range(80))])\n",
    "nonstop = ['pop','—','guitar', 'rock', 'indie', 'black', 'debut', 'indie', 'label', 'solo', 'vocal', 'vocals', 'voice', 'love', 'world', 'live', 'lyrics' ]\n",
    "for word in nonstop:\n",
    "    if(word in stopwords):\n",
    "        stopwords.remove(word)\n",
    "temp = [word.replace('\\'','') for word in stopwords] \n",
    "stopwords |= set(temp)\n",
    "temp = [word.replace('\\'','’') for word in stopwords] \n",
    "stopwords |= set(temp)\n",
    "temp = [word.title() for word in stopwords] \n",
    "stopwords |= set(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
