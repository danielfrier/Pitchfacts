{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import codecs\n",
    "import nltk\n",
    "\n",
    "from nltk import *\n",
    "from nltk.corpus import *\n",
    "from collections import *\n",
    "from gensim import *\n",
    "from gensim.models import *\n",
    "from sklearn import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from nltk.stem import *\n",
    "\n",
    "df = pd.read_csv('p4kreviews.csv', skipinitialspace=True)\n",
    "stopwords= set(stopwords.words('english'))\n",
    "stopwords = [i.encode('ascii','ignore') for i in list(stopwords)]\n",
    "stopwords = set(stopwords)\n",
    "stopwords |= set(['music','album','albums','band','bands','artist','artists','song','songs',\n",
    "                  'track','tracks','time','length','\\\\x','year','years','would','something',\n",
    "                  'time','often','yet','another','title','seems','words','well','really','right',\n",
    "                  'thing','enough','got','know','say','released','people',\n",
    "                  'best','new','reissue','since','less','go','label'])\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [str(re.sub('—',' ',str(df.loc[i].review))).decode('unicode_escape').encode('ascii','ignore') for i in list(range(len(df.loc[:])))]\n",
    "total_text = ' '.join([reviews[i] for i in list(range(len(reviews)))])\n",
    "counter = Counter(total_text.lower().split(' '))\n",
    "ignore = list(stopwords)\n",
    "for word in list(counter):\n",
    "    if word in ignore:\n",
    "        del counter[word]\n",
    "stopwords |= set([counter.most_common(80)[i][0] for i in list(range(80))])\n",
    "nonstop = ['pop','—','guitar', 'rock', 'indie', 'black', 'debut', 'indie', 'label', 'solo', 'vocal', 'vocals', 'voice', 'love', 'world', 'live', 'lyrics' ]\n",
    "for word in nonstop:\n",
    "    if(word in stopwords):\n",
    "        stopwords.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [word.replace('\\'','') for word in stopwords] \n",
    "stopwords |= set(temp)\n",
    "temp = [word.replace('\\'','’') for word in stopwords] \n",
    "stopwords |= set(temp)\n",
    "temp = [word.title() for word in stopwords] \n",
    "stopwords |= set(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(m):\n",
    "    return '' if m.group() in stopwords else m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [ re.sub(r'\\w+(\\-|\\—|\\.|\\&|\\’)?(\\w+)?', remove_stop, review) for review in reviews]\n",
    "reviews = [ re.sub(r'\\-+\\s', ' ', review) for review in reviews]\n",
    "total_text = ' '.join([reviews[i] for i in list(range(len(reviews)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.split('[\\.\\!\\?]\\s',re.sub('[^A-z\\.\\!\\?\\-\\&]+',' ',total_text.lower()))\n",
    "sentences = [sentences[i].split(' ') for i in list(range(len(sentences)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = Word2Vec(sentences, size=800, min_count=20, workers=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.save('wem.wem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = {'doc_id':[],'review':[]}\n",
    "for i in list(range(len(reviews))):\n",
    "    tempreview = str(reviews[i])\n",
    "    tempreview=re.sub('\\S+[\\'\\’\\‘]\\S+','',(tempreview))\n",
    "    tempreview=re.sub('\\w*\\d\\S*','',(tempreview))\n",
    "    tempreview=re.sub('(?<!^|$)(?<!([(\\.)(\\!)(\\?)(\\“)]\\s))([A-Z]\\S+)','',(tempreview))\n",
    "    tempreview=re.sub('[^A-z\\s\\-\\–\\&]',' ',(tempreview))\n",
    "    tempreview=re.sub('\\su\\ss\\s',' U.S. ',(tempreview))\n",
    "    tempreview=re.sub('\\[#.+]\\|+','',(tempreview))\n",
    "    tempreview = tempreview.lower()\n",
    "    tempreview = str(' '.join([lemmatizer.lemmatize(tag[0], get_wordnet_pos(tag[1])) for tag in pos_tag(tempreview.split())]))\n",
    "    rawdata['doc_id'].append(i)\n",
    "    rawdata['review'].append(tempreview)\n",
    "rawdata['review'] = [ re.sub(r'\\w+(\\-|\\—|\\.|\\&|\\’)?(\\w+)?', remove_stop, review) for review in rawdata['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(rawdata,columns=['doc_id','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = df.drop('review',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['doc_id'] = rawdata['doc_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = rawdata['review']\n",
    "texts = [[word for word in document.lower().split() if word not in stopwords]\n",
    "          for document in documents]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "         frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "    for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('reviewsDict.dict')\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('reviewsDict.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
